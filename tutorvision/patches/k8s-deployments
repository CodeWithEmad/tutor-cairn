---
####### Vision plugin
# log collection
# https://vector.dev/docs/setup/installation/platforms/kubernetes/
# https://github.com/timberio/vector/blob/master/distribution/kubernetes/vector-agent/resources.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vision-vector
  labels:
    app.kubernetes.io/name: vision-vector
automountServiceAccountToken: true
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vision-vector
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vision-vector
  labels:
    app.kubernetes.io/name: vision-vector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: vision-vector
subjects:
  - kind: ServiceAccount
    name: vision-vector
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: vision-vector
  labels:
    app.kubernetes.io/name: vision-vector
spec:
  selector:
    matchLabels:
      name: vision-vector
  template:
    metadata:
      labels:
        name: vision-vector
    spec:
      serviceAccountName: vision-vector
      # Run vector next to LMS
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - lms
              topologyKey: kubernetes.io/hostname
      containers:
        - name: vision-vector
          image: docker.io/timberio/vector:0.13.X-alpine
          env:
            - name: VECTOR_SELF_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: VECTOR_SELF_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: VECTOR_SELF_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: PROCFS_ROOT
              value: /host/proc
            - name: SYSFS_ROOT
              value: /host/sys
          volumeMounts:
            - name: var-log
              mountPath: /var/log/
              readOnly: true
            - mountPath: /etc/vector/vector.toml
              name: config
              subPath: vector.toml
              readOnly: true
      volumes:
        - name: var-log
          hostPath:
            path: /var/log/
        - name: config
          configMap:
            name: vision-vector-config
{% if VISION_RUN_CLICKHOUSE %}
---
# data storage
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-clickhouse
  labels:
    app.kubernetes.io/name: vision-clickhouse
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vision-clickhouse
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vision-clickhouse
    spec:
      containers:
        - name: vision-clickhouse
          image: {{ VISION_CLICKHOUSE_DOCKER_IMAGE }}
          volumeMounts:
            - mountPath: /var/lib/clickhouse
              name: data
            - mountPath: /etc/clickhouse-server/users.d/vision.xml
              name: user-config
              subPath: vision.xml
            - mountPath: /scripts/clickhouse-auth.json
              name: clickhouse-auth
              subPath: auth.json
          ports:
            - containerPort: 8123
            - containerPort: 9000
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: vision-clickhouse
        - name: user-config
          configMap:
            name: vision-clickhouse-user-config
        - name: clickhouse-auth
          configMap:
            name: vision-clickhouse-auth
{% endif %}
---
# vision frontend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-superset
  labels:
    app.kubernetes.io/name: vision-superset
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vision-superset
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vision-superset
    spec:
      containers:
        - name: vision-superset
          image: {{ VISION_SUPERSET_DOCKER_IMAGE }}
          volumeMounts:
            - mountPath: /app/superset_config.py
              name: config
              subPath: superset_config.py
            - mountPath: /app/bootstrap/
              name: bootstrap
            - mountPath: /scripts/clickhouse-auth.json
              name: clickhouse-auth
              subPath: auth.json
      volumes:
        - name: config
          configMap:
            name: vision-superset-config
        - name: bootstrap
          configMap:
            name: vision-superset-bootstrap
        - name: clickhouse-auth
          configMap:
            name: vision-clickhouse-auth
---
# frontend worker
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-superset-worker
  labels:
    app.kubernetes.io/name: vision-superset-worker
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vision-superset-worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vision-superset-worker
    spec:
      containers:
        - name: vision-superset-worker
          image: {{ VISION_SUPERSET_DOCKER_IMAGE }}
          args: ["celery", "worker", "--app=superset.tasks.celery_app:app", "-Ofair", "-l", "INFO"]
          volumeMounts:
            - mountPath: /app/superset_config.py
              name: config
              subPath: superset_config.py
      volumes:
        - name: config
          configMap:
            name: vision-superset-config
---
# frontend celery beat
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-superset-worker-beat
  labels:
    app.kubernetes.io/name: vision-superset-worker-beat
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vision-superset-worker-beat
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vision-superset-worker-beat
    spec:
      containers:
        - name: vision-superset-worker-beat
          image: {{ VISION_SUPERSET_DOCKER_IMAGE }}
          args: ["celery", "beat", "--app=superset.tasks.celery_app:app", "--pidfile", "/tmp/celerybeat.pid", "-l", "INFO", "--schedule=/tmp/celerybeat-schedule"]
          volumeMounts:
            - mountPath: /app/superset_config.py
              name: config
              subPath: superset_config.py
      volumes:
        - name: config
          configMap:
            name: vision-superset-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-redis
  labels:
    app.kubernetes.io/name: vision-redis
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vision-redis
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vision-redis
    spec:
      containers:
        - name: vision-superset-worker
          image: docker.io/redis:5.0-alpine
          ports:
            - containerPort: 6379
{% if VISION_RUN_POSTGRESQL %}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-postgresql
  labels:
    app.kubernetes.io/name: vision-postgresql
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vision-postgresql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vision-postgresql
    spec:
      containers:
        - name: vision-postgresql
          image: docker.io/postgres:9.6-alpine
          env:
            - name: POSTGRES_USER
              value: "{{ VISION_POSTGRESQL_USER }}"
            - name: POSTGRES_PASSWORD
              value: "{{ VISION_POSTGRESQL_PASSWORD }}"
            - name: POSTGRES_DB
              value: "{{ VISION_POSTGRESQL_DB }}"
            # The following is required, otherwise postgresql refuses to
            # write to the non-empty directory which contains "lost+found".
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
          ports:
            - containerPort: 5432
          volumeMounts:
            - mountPath: /var/lib/postgresql/data
              name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: vision-postgresql
{% endif %}
